{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be quite complex to gather requirement of customer of product improvement and enhancement when scaling to millions of cutomers and filter them. This simple solution aims to summatize the customer conversations and priotize the requirements with few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ML Approach:**\n",
    "1. Read the data\n",
    "2. Summarize the conversations\n",
    "3. Identify the unmet needs\n",
    "4. Score the needs\n",
    "5. Save results\n",
    "\n",
    "**Output**\n",
    "1. unmet_needs.csv file contains the unmet needs of the customer in the conversation\n",
    "2. result.csv contains entire output with summary and needs\n",
    "\n",
    "**Assumption:**\n",
    "1. All conversations revolve around the same product. \n",
    "2. Scoring of the needs is based on generic business impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install webvtt-py\n",
    "# !pip install tiktoken\n",
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import webvtt\n",
    "import tiktoken\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please extract the zip file and provide the unzip folder as folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input unzipped folder location\n",
    "folder_path = \"transcripts\"\n",
    "# Default OpenAI model used\n",
    "MODEL = \"gpt-3.5-turbo-16k\"\n",
    "# OpenAI key\n",
    "openai.api_key = \"insert-your-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timing(func):\n",
    "    \"\"\"\n",
    "    A decorator to measure the execution time of a function.\n",
    "\n",
    "    Args:\n",
    "        func (callable): The function to be timed.\n",
    "\n",
    "    Returns:\n",
    "        callable: A wrapped function that measures execution time.\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"{func.__name__} took {elapsed_time:.4f} seconds to execute.\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe_to_csv(df, filename, index=False):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to be saved.\n",
    "        filename (str): The name of the CSV file to be created.\n",
    "        index (bool): Whether or not to include the DataFrame index in the CSV file (default is False).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df.to_csv(filename, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing\n",
    "def read_csv_to_dataframe(file_path):\n",
    "    \"\"\"\n",
    "    Read a CSV file into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file to be read.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or None: The DataFrame if successful, None if there was an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: An exception occurred - {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string, encoding_name=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Calculate the number of tokens in a text string using a specified encoding.\n",
    "\n",
    "    Args:\n",
    "        string (str): The input text string.\n",
    "        encoding_name (str): The name of the encoding to use.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of tokens in the input string.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_string_to_dict(json_str):\n",
    "    \"\"\"\n",
    "    Convert a JSON string to a Python dictionary.\n",
    "\n",
    "    Args:\n",
    "        json_str (str): The JSON string to be converted.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: The dictionary representation of the JSON string or None if decoding fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use json.loads() to parse the JSON string into a dictionary\n",
    "        data_dict = json.loads(json_str)\n",
    "        return data_dict\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing\n",
    "def get_completion_from_messages(messages, model=MODEL, temperature=0):\n",
    "    \"\"\"\n",
    "    Generate a completion from a list of messages using the OpenAI GPT-3.5 Turbo model.\n",
    "\n",
    "    Args:\n",
    "        messages (list): A list of message objects in the conversation.\n",
    "        model (str): The name of the GPT-3.5 Turbo model to use (default is None).\n",
    "        temperature (float): The temperature parameter for randomness in text generation (default is 0).\n",
    "\n",
    "    Returns:\n",
    "        str: The generated completion text or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        if response.choices and response.choices[0].message:\n",
    "            return response.choices[0].message[\"content\"]\n",
    "        else:\n",
    "            print(\"Error: No response content received.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lists_to_dataframe(*lists):\n",
    "    \"\"\"\n",
    "    Convert variable number of lists into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        *lists: Variable number of lists.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with columns named after the list variables.\n",
    "    \"\"\"\n",
    "    data = {name: lst for name, lst in zip([var for var in locals() if isinstance(locals()[var], list)], lists)}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vtt_files(folder_path):\n",
    "    \"\"\"\n",
    "    Get a list of .vtt files in the specified folder.\n",
    "\n",
    "    Args:\n",
    "        The path to the folder to search for .vtt files.\n",
    "\n",
    "    Returns:\n",
    "        A list of file paths to .vtt files in the folder.\n",
    "    \"\"\"\n",
    "    # Use a list comprehension to create a list of file paths\n",
    "    files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.lower().endswith(\".vtt\")]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing\n",
    "def read_vtt_files(files):\n",
    "    \"\"\"\n",
    "    Read .vtt files and create a DataFrame.\n",
    "\n",
    "    :param files: List of file paths to .vtt files.\n",
    "    :type files: list\n",
    "    :return: DataFrame containing the parsed data.\n",
    "    :rtype: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    start_times, end_times, captions, identifiers, sources = [], [], [], [], []\n",
    "    \n",
    "    for file in files:\n",
    "        try:\n",
    "            for caption in webvtt.read(file):\n",
    "                start_times.append(caption.start)\n",
    "                end_times.append(caption.end)\n",
    "                captions.append(caption.text)\n",
    "                identifiers.append(caption.identifier)\n",
    "                sources.append(file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file '{file}': {e}\")\n",
    "    \n",
    "    data = {\n",
    "        'start_time': start_times,\n",
    "        'end_time': end_times,\n",
    "        'caption': captions,\n",
    "        'identifier': identifiers,\n",
    "        'source': sources\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_vtt_files took 0.1352 seconds to execute.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>caption</th>\n",
       "      <th>identifier</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3401</th>\n",
       "      <td>00:38:11.594</td>\n",
       "      <td>00:38:14.997</td>\n",
       "      <td>good day and you hope you enjoy\\nyour weekend ...</td>\n",
       "      <td>db45b327-207f-49d4-ac7b-4a7643aee86e-8</td>\n",
       "      <td>transcripts/call-4.vtt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3402</th>\n",
       "      <td>00:38:14.997</td>\n",
       "      <td>00:38:15.699</td>\n",
       "      <td>closer to it.</td>\n",
       "      <td>db45b327-207f-49d4-ac7b-4a7643aee86e-9</td>\n",
       "      <td>transcripts/call-4.vtt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3403</th>\n",
       "      <td>00:38:17.160</td>\n",
       "      <td>00:38:18.250</td>\n",
       "      <td>Thank you, Bernard.</td>\n",
       "      <td>f40fd9f1-c9e5-4012-aff4-6df494e75515-0</td>\n",
       "      <td>transcripts/call-4.vtt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3404</th>\n",
       "      <td>00:38:19.600</td>\n",
       "      <td>00:38:20.200</td>\n",
       "      <td>Personas.</td>\n",
       "      <td>b145f7c4-a6e7-4e71-83ef-f03449bfb101-0</td>\n",
       "      <td>transcripts/call-4.vtt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3405</th>\n",
       "      <td>00:38:22.550</td>\n",
       "      <td>00:38:25.250</td>\n",
       "      <td>Alright, cheers. Bye, bye.</td>\n",
       "      <td>b63b24f8-6717-4e75-814d-3b61dc052e45-0</td>\n",
       "      <td>transcripts/call-4.vtt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        start_time      end_time  \\\n",
       "3401  00:38:11.594  00:38:14.997   \n",
       "3402  00:38:14.997  00:38:15.699   \n",
       "3403  00:38:17.160  00:38:18.250   \n",
       "3404  00:38:19.600  00:38:20.200   \n",
       "3405  00:38:22.550  00:38:25.250   \n",
       "\n",
       "                                                caption  \\\n",
       "3401  good day and you hope you enjoy\\nyour weekend ...   \n",
       "3402                                      closer to it.   \n",
       "3403                                Thank you, Bernard.   \n",
       "3404                                          Personas.   \n",
       "3405                         Alright, cheers. Bye, bye.   \n",
       "\n",
       "                                  identifier                  source  \n",
       "3401  db45b327-207f-49d4-ac7b-4a7643aee86e-8  transcripts/call-4.vtt  \n",
       "3402  db45b327-207f-49d4-ac7b-4a7643aee86e-9  transcripts/call-4.vtt  \n",
       "3403  f40fd9f1-c9e5-4012-aff4-6df494e75515-0  transcripts/call-4.vtt  \n",
       "3404  b145f7c4-a6e7-4e71-83ef-f03449bfb101-0  transcripts/call-4.vtt  \n",
       "3405  b63b24f8-6717-4e75-814d-3b61dc052e45-0  transcripts/call-4.vtt  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data\n",
    "files = get_vtt_files(folder_path)\n",
    "df = read_vtt_files(files)\n",
    "# df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing\n",
    "def preprocess_dataframe(df):\n",
    "    \"\"\"\n",
    "    Preprocess a DataFrame containing captions.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.sort_values(by=['source', 'start_time'])\n",
    "    df['caption'] = df['caption'].astype(str)\n",
    "    df['caption'] = df['caption'].str.replace('\\n', ' ')\n",
    "    df['identifier'] = df['identifier'].apply(lambda x: re.sub(r'-\\d+$', '', x))\n",
    "    mask = df['identifier'] != df['identifier'].shift()\n",
    "    \n",
    "    df['group_id'] = mask.cumsum()\n",
    "    df = df.groupby('group_id').agg({'start_time': 'first', 'end_time': 'last', 'caption': ' '.join, 'identifier': 'first', 'source': 'first'}).reset_index()\n",
    "    df = df.drop('group_id', axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess_dataframe took 0.0582 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "df = preprocess_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condense_caption(df):\n",
    "    \"\"\"\n",
    "    Concatenate caption from the 'caption' column based on the 'source' column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to be processed.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed DataFrame.\n",
    "    \"\"\"\n",
    "    df['caption'] = df.groupby('source')['caption'].transform(' '.join)\n",
    "    df = df.groupby('source').agg({'start_time': 'first', 'end_time': 'last', 'caption': 'first', 'identifier': 'first'}).reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>caption</th>\n",
       "      <th>identifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transcripts/call-1.vtt</td>\n",
       "      <td>00:00:02.240</td>\n",
       "      <td>00:51:16.610</td>\n",
       "      <td>The platform through its. I mean, every other ...</td>\n",
       "      <td>eaba48af-ba4d-4649-8573-02552246990a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>transcripts/call-2.vtt</td>\n",
       "      <td>00:00:02.260</td>\n",
       "      <td>00:29:46.980</td>\n",
       "      <td>Sure. That's your transition to the new platfo...</td>\n",
       "      <td>7fb9629a-5ce5-4f06-b23c-9a766a37533a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transcripts/call-3.vtt</td>\n",
       "      <td>00:00:04.410</td>\n",
       "      <td>00:39:46.980</td>\n",
       "      <td>OK, cool. Um, so as a as, as sort of in terms ...</td>\n",
       "      <td>95b08182-2301-48f1-884d-66b5e5a1f273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transcripts/call-4.vtt</td>\n",
       "      <td>00:00:02.220</td>\n",
       "      <td>00:38:25.250</td>\n",
       "      <td>Directly with the people who build the platfor...</td>\n",
       "      <td>e0968d7f-6ad8-47ec-88c8-4f5bbf9cea9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transcripts/call-5.vtt</td>\n",
       "      <td>00:00:02.780</td>\n",
       "      <td>00:47:49.320</td>\n",
       "      <td>Um, So what I'll do is um. I'll share my scree...</td>\n",
       "      <td>a3b83e0e-077b-4d3a-9d9a-e1ab66edeb2d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   source    start_time      end_time  \\\n",
       "0  transcripts/call-1.vtt  00:00:02.240  00:51:16.610   \n",
       "1  transcripts/call-2.vtt  00:00:02.260  00:29:46.980   \n",
       "2  transcripts/call-3.vtt  00:00:04.410  00:39:46.980   \n",
       "3  transcripts/call-4.vtt  00:00:02.220  00:38:25.250   \n",
       "4  transcripts/call-5.vtt  00:00:02.780  00:47:49.320   \n",
       "\n",
       "                                             caption  \\\n",
       "0  The platform through its. I mean, every other ...   \n",
       "1  Sure. That's your transition to the new platfo...   \n",
       "2  OK, cool. Um, so as a as, as sort of in terms ...   \n",
       "3  Directly with the people who build the platfor...   \n",
       "4  Um, So what I'll do is um. I'll share my scree...   \n",
       "\n",
       "                             identifier  \n",
       "0  eaba48af-ba4d-4649-8573-02552246990a  \n",
       "1  7fb9629a-5ce5-4f06-b23c-9a766a37533a  \n",
       "2  95b08182-2301-48f1-884d-66b5e5a1f273  \n",
       "3  e0968d7f-6ad8-47ec-88c8-4f5bbf9cea9b  \n",
       "4  a3b83e0e-077b-4d3a-9d9a-e1ab66edeb2d  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = condense_caption(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10102\n"
     ]
    }
   ],
   "source": [
    "# Get max number of tokens\n",
    "token_length = df['caption'].apply(num_tokens_from_string)\n",
    "print(max(token_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max token length is less than 16K, hence can use gpt-3.5-turbo-16k. If it were greater, then either would have to select a model with bigger context window or chunk the input text in small batches to fit the context window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unmet_need(instruction, text):\n",
    "    prompt = instruction + text\n",
    "    messages = [{'role': 'user', 'content': prompt}]\n",
    "    response = get_completion_from_messages(messages)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_conversations(df, instruction):\n",
    "    result = []\n",
    "    for conv in df['caption'].tolist():\n",
    "        response_str = get_unmet_need(instruction, conv)\n",
    "        response_dict = json_string_to_dict(response_str)\n",
    "        result.append(response_dict)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction\n",
    "instruction = \"\"\"\n",
    "1. Create a summary of the conversation separated by triple backticks in less than 100 words.\n",
    "2. Identify the unment products needs from the conversation. Provide these needs a score between 1 to 10, where 10 is the highest for business impact on fulflling the need. Provide no more than 5 needs.\n",
    "Please provide a respose in form of json format with 2 keys as 1.'summary', 2.'needs':'score'.\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_completion_from_messages took 8.6151 seconds to execute.\n",
      "get_completion_from_messages took 6.5500 seconds to execute.\n",
      "get_completion_from_messages took 9.4113 seconds to execute.\n",
      "get_completion_from_messages took 7.9129 seconds to execute.\n",
      "get_completion_from_messages took 6.4599 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "result = process_conversations(df, instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary': 'The conversation was about the new platform and its features. The participants discussed the need for a simple user interface, the ability to customize the dashboard, and the importance of clear definitions for data fields. They also mentioned the need for better organization of data and the option to collapse or expand sections. The participants provided feedback on the report builder and suggested improvements for data visualization. They mentioned that they do not use data on the go and prefer to access platforms on laptops or desktops.',\n",
       "  'needs': {'Simple user interface': 8,\n",
       "   'Customizable dashboard': 7,\n",
       "   'Clear definitions for data fields': 9,\n",
       "   'Better organization of data': 6,\n",
       "   'Improved data visualization': 7}},\n",
       " {'summary': 'The conversation was about the new platform and its features. Belinda, an information scientist advisor, discussed her use of the platform for research and development purposes. The platform allows her to access information on competitors, target areas, and indications. The team showcased the new features, such as saved reports, manage columns, and filtering options. Belinda provided positive feedback on the improvements and found the platform easier to use. They also discussed potential visualizations and exporting options. Overall, Belinda expressed satisfaction with the new platform and looked forward to using it.',\n",
       "  'needs': {'Saved reports': 8,\n",
       "   'Manage columns': 7,\n",
       "   'Filtering options': 9,\n",
       "   'Visualizations': 6,\n",
       "   'Exporting options': 5}},\n",
       " {'summary': 'The conversation was a tour of the new features and updates in the Evaluate Pharma platform. The home page now includes saved reports and the ability to access them from the top navigation bar. The manage reports section has improved functionality, including the ability to run multiple saved reports without leaving the page. The filters have been moved to a three-dot menu and can be applied to columns. The data explorer now includes updated data tables and the ability to browse and export data. The drug item pages are centered around the generic name and provide an overview of the compound and its associated products. The platform is being considered for mobile compatibility, and the user expressed interest in being able to export visuals and customize the overview page. The user also mentioned the need for a mobile-friendly version for quick lookups and access to saved reports.',\n",
       "  'needs': {'Mobile compatibility': 7,\n",
       "   'Export visuals': 9,\n",
       "   'Customizable overview page': 8,\n",
       "   'Improved data browsing and exporting': 6,\n",
       "   'Access to saved reports on mobile': 5}},\n",
       " {'summary': 'The conversation was a demonstration of a new platform. The speaker showed the landing page, the ability to access saved reports, and the options for creating new reports. They also discussed the possibility of adding charts and filtering options to make the platform more useful. The speaker asked for feedback on the events and top three sections, and whether mobile access would be beneficial. The participant provided feedback on the usefulness of the platform and suggested improvements for filtering events and adding more insights. They also mentioned that mobile access is not necessary for their needs.',\n",
       "  'needs': {'Filtering events': 8,\n",
       "   'More insights in events section': 7,\n",
       "   'Mobile access': 2,\n",
       "   'Charts for sales data': 9,\n",
       "   'Ability to filter and analyze data at a granular level': 10}},\n",
       " {'summary': 'The conversation was a feedback session on a new platform. The platform is still in development and the customer provided feedback on various aspects such as customization options, data segmentation, and the ability to contact an analyst for assistance. The customer also mentioned the need for better data organization and the ability to easily export data to Excel. Overall, the customer expressed a desire for a more user-friendly and customizable platform.',\n",
       "  'needs': {'Customization options': 8,\n",
       "   'Data segmentation': 7,\n",
       "   'Contact analyst functionality': 9,\n",
       "   'Better data organization': 6,\n",
       "   'Improved data export to Excel': 7}}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(result)\n",
    "result = pd.concat([df, result_df], axis=1)\n",
    "save_dataframe_to_csv(result, 'result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_needs_and_scores(result):\n",
    "    \"\"\"\n",
    "    Extracts 'needs' and 'score' values from a DataFrame column.\n",
    "\n",
    "    Args:\n",
    "        result (pd.DataFrame): The DataFrame containing the 'needs' column.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of two lists, 'needs' and 'score'.\n",
    "    \"\"\"\n",
    "    needs = []\n",
    "    score = []\n",
    "\n",
    "    for needs_dict in result['needs']:\n",
    "        for need, s in needs_dict.items():\n",
    "            needs.append(need)\n",
    "            score.append(s)\n",
    "\n",
    "    return needs, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "needs, score = extract_needs_and_scores(result)\n",
    "\n",
    "# Save unmet needs\n",
    "unmet_need_df = pd.DataFrame({'needs':needs, 'score':score})\n",
    "unmet_need_df.sort_values('score', ascending=False, inplace = True)\n",
    "save_dataframe_to_csv(unmet_need_df, 'unmet_needs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key unmet needs from the product in the conversations are:\n",
      "Ability to filter and analyze data at a granular level\n",
      "Export visuals\n",
      "Clear definitions for data fields\n",
      "Contact analyst functionality\n",
      "Charts for sales data\n"
     ]
    }
   ],
   "source": [
    "# Print top unmet needs\n",
    "top_unmet_needs = unmet_need_df['needs'].head(5).to_list()\n",
    "output = '\\n'.join(top_unmet_needs)\n",
    "print(f\"The key unmet needs from the product in the conversations are:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Further Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**\n",
    "- Text Chunking - make it more robust by chunking the input text into smaller batches, if itoken length exceeds 16k tokens\n",
    "\n",
    "**Engineering**\n",
    "- Modular object Oriented code with unit test. Analyse processing time of functions and improve if required (vectorization, caching, etc.)\n",
    "\n",
    "**Prompt Design**\n",
    "- Prompt Chaining- break down prompts in sub-task and evaluate performance in terms of NLP task and processing time. For e.g., summarize prompt > unmet need extractor > validation\n",
    "- Response Verification - Adding subsequent prompts to verify if the extracted unmet need is indeed present in the conversation\n",
    "\n",
    "**Post-processing**\n",
    "- Group similar extracted features into a one to avoid repetitiveness. Also, that can be used an input feature for scoring.\n",
    "\n",
    "**Evaluation**\n",
    "- Evaluation - Annotate the conversation manually and see how effective the model is.\n",
    "\n",
    "**Model**\n",
    "- Model - explore how results vary with by changing models. For e.g., trying GPT-4"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
